var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"FiniteDiff","category":"page"},{"location":"api/#FiniteDiff","page":"API","title":"FiniteDiff","text":"FiniteDiff\n\nFast non-allocating calculations of gradients, Jacobians, and Hessians with sparsity support.\n\n\n\n\n\n","category":"module"},{"location":"api/#Derivatives","page":"API","title":"Derivatives","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"FiniteDiff.finite_difference_derivative\nFiniteDiff.finite_difference_derivative!\nFiniteDiff.DerivativeCache","category":"page"},{"location":"api/#FiniteDiff.finite_difference_derivative","page":"API","title":"FiniteDiff.finite_difference_derivative","text":"FiniteDiff.finite_difference_derivative(\n    f, x::T,\n    fdtype::Type{T1}=Val{:central},\n    returntype::Type{T2}=eltype(x),\n    f_x::Union{Nothing,T}=nothing)\n\nSingle-point derivative of scalar->scalar maps.\n\n\n\n\n\nFiniteDiff.finite_difference_derivative(\n    f,\n    x          :: AbstractArray{<:Number},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x),      # return type of f\n    fx         :: Union{Nothing,AbstractArray{<:Number}} = nothing,\n    epsilon    :: Union{Nothing,AbstractArray{<:Real}} = nothing;\n    [epsilon_factor])\n\nCompute the derivative df of a scalar-valued map f at a collection of points x.\n\nCache-less.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.finite_difference_derivative!","page":"API","title":"FiniteDiff.finite_difference_derivative!","text":"FiniteDiff.finite_difference_derivative!(\n    df         :: AbstractArray{<:Number},\n    f,\n    x          :: AbstractArray{<:Number},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x),\n    fx         :: Union{Nothing,AbstractArray{<:Number}} = nothing,\n    epsilon    :: Union{Nothing,AbstractArray{<:Real}}   = nothing;\n    [epsilon_factor])\n\nCompute the derivative df of a scalar-valued map f at a collection of points x.\n\nCache-less but non-allocating if fx and epsilon are supplied (fx must be f(x)).\n\n\n\n\n\nFiniteDiff.finite_difference_derivative!(\n    df::AbstractArray{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    cache::DerivativeCache{T1,T2,fdtype,returntype};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    dir=true)\n\nCompute the derivative df of a scalar-valued map f at a collection of points x.\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.DerivativeCache","page":"API","title":"FiniteDiff.DerivativeCache","text":"FiniteDiff.DerivativeCache(\n    x          :: AbstractArray{<:Number},\n    fx         :: Union{Nothing,AbstractArray{<:Number}} = nothing,\n    epsilon    :: Union{Nothing,AbstractArray{<:Real}} = nothing,\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x))\n\nThis allocates either fx or epsilon if these are nothing and they are needed. fx is the current call of f(x) and is required for forward-differencing (otherwise is not necessary).\n\n\n\n\n\n","category":"type"},{"location":"api/#Gradients","page":"API","title":"Gradients","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"FiniteDiff.finite_difference_gradient\nFiniteDiff.finite_difference_gradient!\nFiniteDiff.GradientCache","category":"page"},{"location":"api/#FiniteDiff.finite_difference_gradient","page":"API","title":"FiniteDiff.finite_difference_gradient","text":"FiniteDiff.finite_difference_gradient(\n    f,\n    x,\n    fdtype::Type{T1}=Val{:central},\n    returntype::Type{T2}=eltype(x),\n    inplace::Type{Val{T3}}=Val{true};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    dir=true)\n\nGradients are either a vector->scalar map f(x), or a scalar->vector map f(fx,x) if inplace=Val{true} and fx=f(x) if inplace=Val{false}.\n\nCache-less.\n\n\n\n\n\nFiniteDiff.finite_difference_gradient!(\n    df::AbstractArray{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    cache::GradientCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep\n    dir=true)\n\nGradients are either a vector->scalar map f(x), or a scalar->vector map f(fx,x) if inplace=Val{true} and fx=f(x) if inplace=Val{false}.\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.finite_difference_gradient!","page":"API","title":"FiniteDiff.finite_difference_gradient!","text":"FiniteDiff.finite_difference_gradient!(\n    df,\n    f,\n    x,\n    fdtype::Type{T1}=Val{:central},\n    returntype::Type{T2}=eltype(df),\n    inplace::Type{Val{T3}}=Val{true};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nGradients are either a vector->scalar map f(x), or a scalar->vector map f(fx,x) if inplace=Val{true} and fx=f(x) if inplace=Val{false}.\n\nCache-less.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.GradientCache","page":"API","title":"FiniteDiff.GradientCache","text":"FiniteDiff.GradientCache(\n    df         :: Union{<:Number,AbstractArray{<:Number}},\n    x          :: Union{<:Number, AbstractArray{<:Number}},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(df),\n    inplace    :: Type{Val{T3}} = Val{true})\n\nAllocating Cache Constructor\n\n\n\n\n\nFiniteDiff.GradientCache(\n    fx         :: Union{Nothing,<:Number,AbstractArray{<:Number}},\n    c1         :: Union{Nothing,AbstractArray{<:Number}},\n    c2         :: Union{Nothing,AbstractArray{<:Number}},\n    c3         :: Union{Nothing,AbstractArray{<:Number}},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(fx),\n    inplace    :: Type{Val{T3}} = Val{true})\n\nNon-Allocating Cache Constructor\n\nArguments\n\nfx: Cached function call.\nc1, c2, c3: (Non-aliased) caches for the input vector.\nfdtype = Val(:central): Method for cmoputing the finite difference.\nreturntype = eltype(fx): Element type for the returned function value.\ninplace = Val(false): Whether the function is computed in-place or not.\n\nOutput\n\nThe output is a GradientCache struct.\n\njulia> x = [1.0, 3.0]\n2-element Vector{Float64}:\n 1.0\n 3.0\n\njulia> _f = x -> x[1] + x[2]\n#13 (generic function with 1 method)\n\njulia> fx = _f(x)\n4.0\n\njulia> gradcache = GradientCache(copy(x), copy(x), copy(x), fx)\nGradientCache{Float64, Vector{Float64}, Vector{Float64}, Vector{Float64}, Val{:central}(), Float64, Val{false}()}(4.0, [1.0, 3.0], [1.0, 3.0], [1.0, 3.0])\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"Gradients are either a vector->scalar map f(x), or a scalar->vector map f(fx,x) if inplace=Val{true} and fx=f(x) if inplace=Val{false}.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Note that here fx is a cached function call of f. If you provide fx, then fx will be used in the forward differencing method to skip a function call. It is on you to make sure that you update cache.fx every time before calling FiniteDiff.finite_difference_gradient!. If fx is an immutable, e.g. a scalar or  a StaticArray, cache.fx should be updated using @set from Setfield.jl. A good use of this is if you have a cache array for the output of fx already being used, you can make it alias into the differencing algorithm here.","category":"page"},{"location":"api/#Jacobians","page":"API","title":"Jacobians","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"FiniteDiff.finite_difference_jacobian\nFiniteDiff.finite_difference_jacobian!\nFiniteDiff.JacobianCache","category":"page"},{"location":"api/#FiniteDiff.finite_difference_jacobian","page":"API","title":"FiniteDiff.finite_difference_jacobian","text":"FiniteDiff.finite_difference_jacobian(\n    f,\n    x          :: AbstractArray{<:Number},\n    fdtype     :: Type{T1}=Val{:central},\n    returntype :: Type{T2}=eltype(x),\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec = 1:length(x),\n    sparsity = nothing,\n    jac_prototype = nothing,\n    dir=true)\n\nCache-less.\n\n\n\n\n\nFiniteDiff.finite_difference_jacobian(\n    f,\n    x,\n    cache::JacobianCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec = cache.colorvec,\n    sparsity = cache.sparsity,\n    jac_prototype = nothing,\n    dir=true)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.finite_difference_jacobian!","page":"API","title":"FiniteDiff.finite_difference_jacobian!","text":"finite_difference_jacobian!(\n    J::AbstractMatrix,\n    f,\n    x::AbstractArray{<:Number},\n    fdtype     :: Type{T1}=Val{:forward},\n    returntype :: Type{T2}=eltype(x),\n    f_in       :: Union{T2,Nothing}=nothing;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec = 1:length(x),\n    sparsity = ArrayInterfaceCore.has_sparsestruct(J) ? J : nothing)\n\nCache-less.\n\n\n\n\n\nFiniteDiff.finite_difference_jacobian!(\n    J::AbstractMatrix{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    cache::JacobianCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec = cache.colorvec,\n    sparsity = cache.sparsity,\n    dir=true)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.JacobianCache","page":"API","title":"FiniteDiff.JacobianCache","text":"FiniteDiff.JacobianCache(\n    x,\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x),\n    colorvec = 1:length(x)\n    sparsity = nothing)\n\nAllocating Cache Constructor.\n\nThis assumes the Jacobian is square.\n\n\n\n\n\nFiniteDiff.JacobianCache(\n    x1 ,\n    fx ,\n    fx1,\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(fx),\n    colorvec = 1:length(x1),\n    sparsity = nothing)\n\nNon-Allocating Cache Constructor.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"Jacobians are for functions f!(fx,x) when using in-place finite_difference_jacobian!, and fx = f(x) when using out-of-place finite_difference_jacobian. The out-of-place jacobian will return a similar type as jac_prototype if it is not a nothing. For non-square Jacobians, a cache which specifies the vector fx is required.","category":"page"},{"location":"api/","page":"API","title":"API","text":"For sparse differentiation, pass a colorvec of matrix colors. sparsity should be a sparse or structured matrix (Tridiagonal, Banded, etc. according to the ArrayInterfaceCore.jl specs) to allow for decompression, otherwise the result will be the colorvec compressed Jacobian.","category":"page"},{"location":"api/#Hessians","page":"API","title":"Hessians","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"FiniteDiff.finite_difference_hessian\nFiniteDiff.finite_difference_hessian!\nFiniteDiff.HessianCache","category":"page"},{"location":"api/#FiniteDiff.finite_difference_hessian","page":"API","title":"FiniteDiff.finite_difference_hessian","text":"finite_difference_hessian(\n    f,\n    x::AbstractArray{<:Number},\n    fdtype     :: Type{T1}=Val{:hcentral},\n    inplace    :: Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCache-less.\n\n\n\n\n\nfinite_difference_hessian(\n    f,\n    x,\n    cache::HessianCache{T,fdtype,inplace};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.finite_difference_hessian!","page":"API","title":"FiniteDiff.finite_difference_hessian!","text":"finite_difference_hessian!(\n    H::AbstractMatrix,\n    f,\n    x::AbstractArray{<:Number},\n    fdtype     :: Type{T1}=Val{:hcentral},\n    inplace    :: Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCache-less.\n\n\n\n\n\nfinite_difference_hessian!(\n    H,\n    f,\n    x,\n    cache::HessianCache{T,fdtype,inplace};\n    relstep = default_relstep(fdtype, eltype(x)),\n    absstep = relstep)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"api/#FiniteDiff.HessianCache","page":"API","title":"FiniteDiff.HessianCache","text":"HessianCache(\n    xpp,\n    xpm,\n    xmp,\n    xmm,\n    fdtype::Type{T1}=Val{:hcentral},\n    inplace::Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false})\n\nNon-allocating cache constructor.\n\n\n\n\n\nHessianCache(\n    x,\n    fdtype::Type{T1}=Val{:hcentral},\n    inplace::Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false})\n\nAllocating cache constructor.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"Hessians are for functions f(x) which return a scalar.","category":"page"},{"location":"tutorials/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/#Fast-Dense-Jacobians","page":"Tutorials","title":"Fast Dense Jacobians","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"It's always fun to start out with a tutorial before jumping into the details! Suppose we had the functions:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using FiniteDiff, StaticArrays\n\nfcalls = 0\nfunction f(dx,x) # in-place\n  global fcalls += 1\n  for i in 2:length(x)-1\n    dx[i] = x[i-1] - 2x[i] + x[i+1]\n  end\n  dx[1] = -2x[1] + x[2]\n  dx[end] = x[end-1] - 2x[end]\n  nothing\nend\n\nconst N = 10\nhandleleft(x,i) = i==1 ? zero(eltype(x)) : x[i-1]\nhandleright(x,i) = i==length(x) ? zero(eltype(x)) : x[i+1]\nfunction g(x) # out-of-place\n  global fcalls += 1\n  @SVector [handleleft(x,i) - 2x[i] + handleright(x,i) for i in 1:N]\nend","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"and we wanted to calculate the derivatives of them. The simplest thing we can do is ask for the Jacobian. If we want to allocate the result, we'd use the allocating function finite_difference_jacobian on a 1-argument function g:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"x = @SVector rand(N)\nFiniteDiff.finite_difference_jacobian(g,x)\n\n#=\n10×10 SArray{Tuple{10,10},Float64,2,100} with indices SOneTo(10)×SOneTo(10):\n -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0\n=#","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"FiniteDiff.jl assumes you're a smart cookie, and so if you used an out-of-place function then it'll not mutate vectors at all, and is thus compatible with objects like StaticArrays and will give you a fast Jacobian.","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"But if you wanted to use mutation, then we'd have to use the in-place function f and call the mutating form:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"x = rand(10)\noutput = zeros(10,10)\nFiniteDiff.finite_difference_jacobian!(output,f,x)\noutput\n\n#=\n10×10 Array{Float64,2}:\n -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0\n=#","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"But what if you want this to be completely non-allocating on your mutating form? Then you need to preallocate a cache:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"cache = FiniteDiff.JacobianCache(x)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"and now using this cache avoids allocating:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"@time FiniteDiff.finite_difference_jacobian!(output,f,x,cache) # 0.000008 seconds (7 allocations: 224 bytes)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"And that's pretty much it! Gradients and Hessians work similarly: out of place doesn't index, and in-place avoids allocations. Either way, you're fast. GPUs etc. all work.","category":"page"},{"location":"tutorials/#Fast-Sparse-Jacobians","page":"Tutorials","title":"Fast Sparse Jacobians","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Now let's exploit sparsity. If we knew the sparsity pattern we could write it down analytically as a sparse matrix, but let's assume we don't. Thus we can use SparsityDetection.jl to automatically get the sparsity pattern of the Jacobian as a sparse matrix:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using SparsityDetection, SparseArrays\nin = rand(10)\nout = similar(in)\nsparsity_pattern = sparsity!(f,out,in)\nsparsejac = Float64.(sparse(sparsity_pattern))","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Then we can use SparseDiffTools.jl to get the color vector:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using SparseDiffTools\ncolors = matrix_colors(sparsejac)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Now we can do sparse differentiation by passing the color vector and the sparsity pattern:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"sparsecache = FiniteDiff.JacobianCache(x,colorvec=colors,sparsity=sparsejac)\nFiniteDiff.finite_difference_jacobian!(sparsejac,f,x,sparsecache)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Note that the number of f evaluations to fill a Jacobian is 1+maximum(colors). By default, colors=1:length(x), so in this case we went from 10 function calls to 4. The sparser the matrix, the more the gain! We can measure this as well:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"fcalls = 0\nFiniteDiff.finite_difference_jacobian!(output,f,x,cache)\nfcalls #11\n\nfcalls = 0\nFiniteDiff.finite_difference_jacobian!(sparsejac,f,x,sparsecache)\nfcalls #4","category":"page"},{"location":"tutorials/#Fast-Tridiagonal-Jacobians","page":"Tutorials","title":"Fast Tridiagonal Jacobians","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Handling dense matrices? Easy. Handling sparse matrices? Cool stuff. Automatically specializing on the exact structure of a matrix? Even better. FiniteDiff can specialize on types which implement the ArrayInterfaceCore.jl interface. This includes:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Diagonal\nBidiagonal\nUpperTriangular and LowerTriangular\nTridiagonal and SymTridiagonal\nBandedMatrices.jl\nBlockBandedMatrices.jl","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Our previous example had a Tridiagonal Jacobian, so let's use this. If we just do","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using ArrayInterfaceCore, LinearAlgebra\ntridiagjac = Tridiagonal(output)\ncolors = matrix_colors(jac)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"we get the analytical solution to the optimal matrix colors for our structured Jacobian. Now we can use this in our differencing routines:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"tridiagcache = FiniteDiff.JacobianCache(x,colorvec=colors,sparsity=tridiagjac)\nFiniteDiff.finite_difference_jacobian!(tridiagjac,f,x,tridiagcache)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"It'll use a special iteration scheme dependent on the matrix type to accelerate it beyond general sparse usage.","category":"page"},{"location":"tutorials/#Fast-Block-Banded-Matrices","page":"Tutorials","title":"Fast Block Banded Matrices","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Now let's showcase a difficult example. Say we had a large system of partial differential equations, with a function like:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"function pde(out, x)\n\tx = reshape(x, 100, 100)\n\tout = reshape(out, 100, 100)\n\tfor i in 1:100\n\t\tfor j in 1:100\n\t\t\tout[i, j] = x[i, j] + x[max(i -1, 1), j] + x[min(i+1, size(x, 1)), j] +  x[i, max(j-1, 1)]  + x[i, min(j+1, size(x, 2))]\n\t\tend\n\tend\n\treturn vec(out)\nend\nx = rand(10000)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"In this case, we can see that our sparsity pattern is a BlockBandedMatrix, so let's specialize the Jacobian calculation on this fact:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using FillArrays, BlockBandedMatrices\nJbbb = BandedBlockBandedMatrix(Ones(10000, 10000), fill(100, 100), fill(100, 100), (1, 1), (1, 1))\ncolorsbbb = ArrayInterfaceCore.matrix_colors(Jbbb)\nbbbcache = FiniteDiff.JacobianCache(x,colorvec=colorsbbb,sparsity=Jbbb)\nFiniteDiff.finite_difference_jacobian!(Jbbb, pde, x, bbbcache)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"And boom, a fast Jacobian filling algorithm on your special matrix.","category":"page"},{"location":"reproducibility/#Reproducibility","page":"-","title":"Reproducibility","text":"","category":"section"},{"location":"reproducibility/","page":"-","title":"-","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"</details>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"</details>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"</details>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"You can also download the \n<a href=\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"\">project</a> file.","category":"page"},{"location":"","page":"Home","title":"Home","text":"EditURL = \"https://github.com/JuliaDiff/FiniteDiff.jl/blob/master/README.md\"","category":"page"},{"location":"#FiniteDiff","page":"Home","title":"FiniteDiff","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Join the chat at https://julialang.zulipchat.com #sciml-bridged) (Image: Global Docs)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: codecov) (Image: Build Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages) (Image: SciML Code Style)","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is for calculating derivatives, gradients, Jacobians, Hessians, etc. numerically. This library is for maximizing speed while giving a usable interface to end users in a way that specializes on array types and sparsity. Included is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fully non-allocating mutable forms for fast array support\nFully non-mutating forms for static array support\nColoring vectors for efficient calculation of sparse Jacobians\nGPU-compatible, to the extent that you can be with finite differencing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want the fastest versions, create a cache and repeatedly call the differencing functions at different x values (or with different f functions), while if you want a quick and dirty numerical answer, directly call a differencing function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For analogous sparse differentiation with automatic differentiation, see SparseDiffTools.jl.","category":"page"},{"location":"#FiniteDiff.jl-vs-FiniteDifferences.jl","page":"Home","title":"FiniteDiff.jl vs FiniteDifferences.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FiniteDiff.jl and FiniteDifferences.jl are similar libraries: both calculate approximate derivatives numerically. You should definitely use one or the other, rather than the legacy Calculus.jl finite differencing, or reimplementing it yourself. At some point in the future they might merge, or one might depend on the other. Right now here are the differences:","category":"page"},{"location":"","page":"Home","title":"Home","text":"FiniteDifferences.jl supports basically any type, where as FiniteDiff.jl supports only array-ish types\nFiniteDifferences.jl supports higher order approximation\nFiniteDiff.jl is carefully optimized to minimize allocations\nFiniteDiff.jl supports coloring vectors for efficient calculation of sparse Jacobians","category":"page"},{"location":"#General-structure","page":"Home","title":"General structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The general structure of the library is as follows. You can call the differencing functions directly and this will allocate a temporary cache to solve the problem with. To make this non-allocating for repeat calls, you can call the cache construction functions. Each cache construction function has two possibilities: one version where you give it prototype arrays and it generates the cache variables, and one fully non-allocating version where you give it the cache variables. This is summarized as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Just want a quick derivative? Calculating once? Call the differencing function.\nGoing to calculate the derivative multiple times but don't have cache arrays around? Use the allocating cache and then pass this into the differencing function (this will allocate only in the one cache construction).\nHave cache variables around from your own algorithm and want to re-use them in the differencing functions? Use the non-allocating cache construction and pass the cache to the differencing function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the documentation for details on the API.","category":"page"},{"location":"#Function-definitions","page":"Home","title":"Function definitions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In all functions, the inplace form is f!(dx,x) while the out of place form is dx = f(x).","category":"page"},{"location":"#Coloring-vectors","page":"Home","title":"Coloring vectors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Coloring vectors are allowed to be supplied to the Jacobian routines, and these are the directional derivatives for constructing the Jacobian. For example, an accurate NxN tridiagonal Jacobian can be computed in just 4 f calls by using colorvec=repeat(1:3,N÷3). For information on automatically generating coloring vectors of sparse matrices, see SparseDiffTools.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Hessian coloring support is coming soon!","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"Home","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can also download the \n<a href=\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"\">project</a> file.","category":"page"}]
}
