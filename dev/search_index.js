var documenterSearchIndex = {"docs":
[{"location":"derivatives/#Derivatives","page":"Derivatives","title":"Derivatives","text":"","category":"section"},{"location":"derivatives/","page":"Derivatives","title":"Derivatives","text":"Functions for computing derivatives of scalar-valued functions.","category":"page"},{"location":"derivatives/#Overview","page":"Derivatives","title":"Overview","text":"","category":"section"},{"location":"derivatives/","page":"Derivatives","title":"Derivatives","text":"Derivatives are computed for scalar→scalar maps f(x) where x can be a single point or a collection of points. The derivative functions support:","category":"page"},{"location":"derivatives/","page":"Derivatives","title":"Derivatives","text":"Forward differences: O(1) function evaluation per point, O(h) accuracy\nCentral differences: O(2) function evaluations per point, O(h²) accuracy  \nComplex step: O(1) function evaluation per point, machine precision accuracy","category":"page"},{"location":"derivatives/","page":"Derivatives","title":"Derivatives","text":"For optimal performance with repeated computations, use the cached versions with DerivativeCache.","category":"page"},{"location":"derivatives/#Functions","page":"Derivatives","title":"Functions","text":"","category":"section"},{"location":"derivatives/","page":"Derivatives","title":"Derivatives","text":"FiniteDiff.finite_difference_derivative\nFiniteDiff.finite_difference_derivative!","category":"page"},{"location":"derivatives/#FiniteDiff.finite_difference_derivative","page":"Derivatives","title":"FiniteDiff.finite_difference_derivative","text":"FiniteDiff.finite_difference_derivative(\n    f, x::T,\n    fdtype::Type{T1}=Val{:central},\n    returntype::Type{T2}=eltype(x),\n    f_x::Union{Nothing,T}=nothing)\n\nSingle-point derivative of scalar->scalar maps.\n\n\n\n\n\nFiniteDiff.finite_difference_derivative(\n    f,\n    x          :: AbstractArray{<:Number},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x),      # return type of f\n    fx         :: Union{Nothing,AbstractArray{<:Number}} = nothing,\n    epsilon    :: Union{Nothing,AbstractArray{<:Real}} = nothing;\n    [epsilon_factor])\n\nCompute the derivative df of a scalar-valued map f at a collection of points x.\n\nCache-less.\n\n\n\n\n\n","category":"function"},{"location":"derivatives/#FiniteDiff.finite_difference_derivative!","page":"Derivatives","title":"FiniteDiff.finite_difference_derivative!","text":"FiniteDiff.finite_difference_derivative!(\n    df         :: AbstractArray{<:Number},\n    f,\n    x          :: AbstractArray{<:Number},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x),\n    fx         :: Union{Nothing,AbstractArray{<:Number}} = nothing,\n    epsilon    :: Union{Nothing,AbstractArray{<:Real}}   = nothing;\n    [epsilon_factor])\n\nCompute the derivative df of a scalar-valued map f at a collection of points x.\n\nCache-less but non-allocating if fx and epsilon are supplied (fx must be f(x)).\n\n\n\n\n\nFiniteDiff.finite_difference_derivative!(\n    df::AbstractArray{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    cache::DerivativeCache{T1,T2,fdtype,returntype};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    dir=true)\n\nCompute the derivative df of a scalar-valued map f at a collection of points x.\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"derivatives/#Cache","page":"Derivatives","title":"Cache","text":"","category":"section"},{"location":"derivatives/","page":"Derivatives","title":"Derivatives","text":"FiniteDiff.DerivativeCache","category":"page"},{"location":"derivatives/#FiniteDiff.DerivativeCache","page":"Derivatives","title":"FiniteDiff.DerivativeCache","text":"FiniteDiff.DerivativeCache(\n    x          :: AbstractArray{<:Number},\n    fx         :: Union{Nothing,AbstractArray{<:Number}} = nothing,\n    epsilon    :: Union{Nothing,AbstractArray{<:Real}} = nothing,\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x))\n\nThis allocates either fx or epsilon if these are nothing and they are needed. fx is the current call of f(x) and is required for forward-differencing (otherwise is not necessary).\n\n\n\n\n\n","category":"type"},{"location":"jacobians/#Jacobians","page":"Jacobians","title":"Jacobians","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"Functions for computing Jacobian matrices of vector-valued functions.","category":"page"},{"location":"jacobians/#Function-Types","page":"Jacobians","title":"Function Types","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"Jacobians support the following function signatures:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"Out-of-place: fx = f(x) where both x and fx are vectors\nIn-place: f!(fx, x) where f! modifies fx in-place","category":"page"},{"location":"jacobians/#Sparse-Jacobians","page":"Jacobians","title":"Sparse Jacobians","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"FiniteDiff.jl provides efficient sparse Jacobian computation using graph coloring:","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"Pass a colorvec of matrix colors to enable column compression\nProvide sparsity as a sparse or structured matrix (Tridiagonal, Banded, etc.)\nSupports automatic sparsity pattern detection via ArrayInterfaceCore.jl\nResults are automatically decompressed unless sparsity=nothing","category":"page"},{"location":"jacobians/#Performance-Notes","page":"Jacobians","title":"Performance Notes","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"Forward differences: O(n) function evaluations, O(h) accuracy  \nCentral differences: O(2n) function evaluations, O(h²) accuracy\nComplex step: O(n) function evaluations, machine precision accuracy\nSparse Jacobians: Use graph coloring to reduce function evaluations significantly","category":"page"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"For non-square Jacobians, specify the output vector fx when creating the cache to ensure proper sizing.","category":"page"},{"location":"jacobians/#Functions","page":"Jacobians","title":"Functions","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"FiniteDiff.finite_difference_jacobian\nFiniteDiff.finite_difference_jacobian!","category":"page"},{"location":"jacobians/#FiniteDiff.finite_difference_jacobian","page":"Jacobians","title":"FiniteDiff.finite_difference_jacobian","text":"FiniteDiff.finite_difference_jacobian(\n    f,\n    x::AbstractArray{<:Number},\n    fdtype::Type{T1}=Val{:forward},\n    returntype::Type{T2}=eltype(x),\n    f_in=nothing;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec=1:length(x),\n    sparsity=nothing,\n    jac_prototype=nothing,\n    dir=true)\n\nCompute the Jacobian matrix of function f at point x using finite differences.\n\nThis is the cache-less version that allocates temporary arrays internally.  The Jacobian J[i,j] = ∂f[i]/∂x[j] is computed using the specified finite  difference method. Supports sparse Jacobians via graph coloring.\n\nArguments\n\nf: Function to differentiate (vector→vector map)\nx::AbstractArray{<:Number}: Point at which to evaluate the Jacobian\nfdtype::Type{T1}=Val{:forward}: Finite difference method (:forward, :central, :complex)\nreturntype::Type{T2}=eltype(x): Element type of the function output\nf_in=nothing: Pre-computed f(x) value (if available)\n\nKeyword Arguments\n\nrelstep: Relative step size (default: method-dependent optimal value)\nabsstep=relstep: Absolute step size fallback\ncolorvec=1:length(x): Column coloring for sparse Jacobians  \nsparsity=nothing: Sparsity pattern for the Jacobian\njac_prototype=nothing: Prototype matrix defining Jacobian structure\ndir=true: Direction for step size (typically ±1)\n\nReturns\n\nJacobian matrix J where J[i,j] = ∂f[i]/∂x[j]\n\nExamples\n\nf(x) = [x[1]^2 + x[2], x[1] * x[2]]\nx = [1.0, 2.0]\nJ = finite_difference_jacobian(f, x)  # 2×2 matrix\n\nNotes\n\nForward differences: O(n) function evaluations, O(h) accuracy\nCentral differences: O(2n) function evaluations, O(h²) accuracy  \nComplex step: O(n) function evaluations, machine precision accuracy\nSparse Jacobians use graph coloring to reduce function evaluations\n\n\n\n\n\nFiniteDiff.finite_difference_jacobian(\n    f,\n    x,\n    cache::JacobianCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec = cache.colorvec,\n    sparsity = cache.sparsity,\n    jac_prototype = nothing,\n    dir=true)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"jacobians/#FiniteDiff.finite_difference_jacobian!","page":"Jacobians","title":"FiniteDiff.finite_difference_jacobian!","text":"finite_difference_jacobian!(\n    J::AbstractMatrix,\n    f,\n    x::AbstractArray{<:Number},\n    fdtype     :: Type{T1}=Val{:forward},\n    returntype :: Type{T2}=eltype(x),\n    f_in       :: Union{T2,Nothing}=nothing;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec = 1:length(x),\n    sparsity = ArrayInterfaceCore.has_sparsestruct(J) ? J : nothing)\n\nCache-less.\n\n\n\n\n\nFiniteDiff.finite_difference_jacobian!(\n    J::AbstractMatrix{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    cache::JacobianCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    colorvec = cache.colorvec,\n    sparsity = cache.sparsity,\n    dir=true)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"jacobians/#Cache","page":"Jacobians","title":"Cache","text":"","category":"section"},{"location":"jacobians/","page":"Jacobians","title":"Jacobians","text":"FiniteDiff.JacobianCache","category":"page"},{"location":"jacobians/#FiniteDiff.JacobianCache","page":"Jacobians","title":"FiniteDiff.JacobianCache","text":"FiniteDiff.JacobianCache(\n    x,\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(x),\n    colorvec = 1:length(x)\n    sparsity = nothing)\n\nAllocating Cache Constructor.\n\nThis assumes the Jacobian is square.\n\n\n\n\n\nFiniteDiff.JacobianCache(\n    x1 ,\n    fx ,\n    fx1,\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(fx),\n    colorvec = 1:length(x1),\n    sparsity = nothing)\n\nNon-Allocating Cache Constructor.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/#Fast-Dense-Jacobians","page":"Tutorials","title":"Fast Dense Jacobians","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"It's always fun to start out with a tutorial before jumping into the details! Suppose we had the functions:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using FiniteDiff, StaticArrays\n\nfcalls = 0\nfunction f(dx,x) # in-place\n  global fcalls += 1\n  for i in 2:length(x)-1\n    dx[i] = x[i-1] - 2x[i] + x[i+1]\n  end\n  dx[1] = -2x[1] + x[2]\n  dx[end] = x[end-1] - 2x[end]\n  nothing\nend\n\nconst N = 10\nhandleleft(x,i) = i==1 ? zero(eltype(x)) : x[i-1]\nhandleright(x,i) = i==length(x) ? zero(eltype(x)) : x[i+1]\nfunction g(x) # out-of-place\n  global fcalls += 1\n  @SVector [handleleft(x,i) - 2x[i] + handleright(x,i) for i in 1:N]\nend","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"and we wanted to calculate the derivatives of them. The simplest thing we can do is ask for the Jacobian. If we want to allocate the result, we'd use the allocating function finite_difference_jacobian on a 1-argument function g:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"x = @SVector rand(N)\nFiniteDiff.finite_difference_jacobian(g,x)\n\n#=\n10×10 SArray{Tuple{10,10},Float64,2,100} with indices SOneTo(10)×SOneTo(10):\n -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0\n=#","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"FiniteDiff.jl assumes you're a smart cookie, and so if you used an out-of-place function then it'll not mutate vectors at all, and is thus compatible with objects like StaticArrays and will give you a fast Jacobian.","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"But if you wanted to use mutation, then we'd have to use the in-place function f and call the mutating form:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"x = rand(10)\noutput = zeros(10,10)\nFiniteDiff.finite_difference_jacobian!(output,f,x)\noutput\n\n#=\n10×10 Array{Float64,2}:\n -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0   0.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0   1.0\n  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  -2.0\n=#","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"But what if you want this to be completely non-allocating on your mutating form? Then you need to preallocate a cache:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"cache = FiniteDiff.JacobianCache(x)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"and now using this cache avoids allocating:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"@time FiniteDiff.finite_difference_jacobian!(output,f,x,cache) # 0.000008 seconds (7 allocations: 224 bytes)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"And that's pretty much it! Gradients and Hessians work similarly: out of place doesn't index, and in-place avoids allocations. Either way, you're fast. GPUs etc. all work.","category":"page"},{"location":"tutorials/#Fast-Sparse-Jacobians","page":"Tutorials","title":"Fast Sparse Jacobians","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Now let's exploit sparsity. If we knew the sparsity pattern we could write it down analytically as a sparse matrix, but let's assume we don't. Thus we can use SparsityDetection.jl to automatically get the sparsity pattern of the Jacobian as a sparse matrix:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using SparsityDetection, SparseArrays\nin = rand(10)\nout = similar(in)\nsparsity_pattern = sparsity!(f,out,in)\nsparsejac = Float64.(sparse(sparsity_pattern))","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Then we can use SparseDiffTools.jl to get the color vector:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using SparseDiffTools\ncolors = matrix_colors(sparsejac)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Now we can do sparse differentiation by passing the color vector and the sparsity pattern:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"sparsecache = FiniteDiff.JacobianCache(x,colorvec=colors,sparsity=sparsejac)\nFiniteDiff.finite_difference_jacobian!(sparsejac,f,x,sparsecache)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Note that the number of f evaluations to fill a Jacobian is 1+maximum(colors). By default, colors=1:length(x), so in this case we went from 10 function calls to 4. The sparser the matrix, the more the gain! We can measure this as well:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"fcalls = 0\nFiniteDiff.finite_difference_jacobian!(output,f,x,cache)\nfcalls #11\n\nfcalls = 0\nFiniteDiff.finite_difference_jacobian!(sparsejac,f,x,sparsecache)\nfcalls #4","category":"page"},{"location":"tutorials/#Fast-Tridiagonal-Jacobians","page":"Tutorials","title":"Fast Tridiagonal Jacobians","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Handling dense matrices? Easy. Handling sparse matrices? Cool stuff. Automatically specializing on the exact structure of a matrix? Even better. FiniteDiff can specialize on types which implement the ArrayInterfaceCore.jl interface. This includes:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Diagonal\nBidiagonal\nUpperTriangular and LowerTriangular\nTridiagonal and SymTridiagonal\nBandedMatrices.jl\nBlockBandedMatrices.jl","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Our previous example had a Tridiagonal Jacobian, so let's use this. If we just do","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using ArrayInterfaceCore, LinearAlgebra\ntridiagjac = Tridiagonal(output)\ncolors = matrix_colors(jac)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"we get the analytical solution to the optimal matrix colors for our structured Jacobian. Now we can use this in our differencing routines:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"tridiagcache = FiniteDiff.JacobianCache(x,colorvec=colors,sparsity=tridiagjac)\nFiniteDiff.finite_difference_jacobian!(tridiagjac,f,x,tridiagcache)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"It'll use a special iteration scheme dependent on the matrix type to accelerate it beyond general sparse usage.","category":"page"},{"location":"tutorials/#Fast-Block-Banded-Matrices","page":"Tutorials","title":"Fast Block Banded Matrices","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Now let's showcase a difficult example. Say we had a large system of partial differential equations, with a function like:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"function pde(out, x)\n\tx = reshape(x, 100, 100)\n\tout = reshape(out, 100, 100)\n\tfor i in 1:100\n\t\tfor j in 1:100\n\t\t\tout[i, j] = x[i, j] + x[max(i -1, 1), j] + x[min(i+1, size(x, 1)), j] +  x[i, max(j-1, 1)]  + x[i, min(j+1, size(x, 2))]\n\t\tend\n\tend\n\treturn vec(out)\nend\nx = rand(10000)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"In this case, we can see that our sparsity pattern is a BlockBandedMatrix, so let's specialize the Jacobian calculation on this fact:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"using FillArrays, BlockBandedMatrices\nJbbb = BandedBlockBandedMatrix(Ones(10000, 10000), fill(100, 100), fill(100, 100), (1, 1), (1, 1))\ncolorsbbb = ArrayInterfaceCore.matrix_colors(Jbbb)\nbbbcache = FiniteDiff.JacobianCache(x,colorvec=colorsbbb,sparsity=Jbbb)\nFiniteDiff.finite_difference_jacobian!(Jbbb, pde, x, bbbcache)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"And boom, a fast Jacobian filling algorithm on your special matrix.","category":"page"},{"location":"jvp/#Jacobian-Vector-Products-(JVP)","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products (JVP)","text":"","category":"section"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"Functions for computing Jacobian-vector products efficiently without forming the full Jacobian matrix.","category":"page"},{"location":"jvp/#Mathematical-Background","page":"Jacobian-Vector Products","title":"Mathematical Background","text":"","category":"section"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"The JVP computes J(x) * v where J(x) is the Jacobian of function f at point x and v is a direction vector. This is computed using finite difference approximations:","category":"page"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"Forward: J(x) * v ≈ (f(x + h*v) - f(x)) / h  \nCentral: J(x) * v ≈ (f(x + h*v) - f(x - h*v)) / (2h)","category":"page"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"where h is the step size and v is the direction vector.","category":"page"},{"location":"jvp/#Performance-Benefits","page":"Jacobian-Vector Products","title":"Performance Benefits","text":"","category":"section"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"JVP functions are particularly efficient when you only need directional derivatives:","category":"page"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"Function evaluations: Only 2 function evaluations (vs O(n) for full Jacobian)\nForward differences: 2 function evaluations, O(h) accuracy\nCentral differences: 2 function evaluations, O(h²) accuracy  \nMemory efficient: No need to store the full Jacobian matrix","category":"page"},{"location":"jvp/#Use-Cases","page":"Jacobian-Vector Products","title":"Use Cases","text":"","category":"section"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"JVP is particularly useful for:","category":"page"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"Optimization: Computing directional derivatives along search directions\nSparse directions: When v has few non-zero entries\nMemory constraints: Avoiding storage of large Jacobian matrices\nNewton methods: Computing Newton steps J⁻¹ * v iteratively","category":"page"},{"location":"jvp/#Limitations","page":"Jacobian-Vector Products","title":"Limitations","text":"","category":"section"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"Complex step: JVP does not currently support complex step differentiation (Val(:complex))\nIn-place functions: For in-place function evaluation, ensure proper cache sizing","category":"page"},{"location":"jvp/#Functions","page":"Jacobian-Vector Products","title":"Functions","text":"","category":"section"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"FiniteDiff.finite_difference_jvp\nFiniteDiff.finite_difference_jvp!","category":"page"},{"location":"jvp/#FiniteDiff.finite_difference_jvp","page":"Jacobian-Vector Products","title":"FiniteDiff.finite_difference_jvp","text":"FiniteDiff.finite_difference_jvp(\n    f,\n    x::AbstractArray{<:Number},\n    v::AbstractArray{<:Number},\n    fdtype::Type{T1}=Val{:forward},\n    f_in=nothing;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCompute the Jacobian-vector product J(x) * v using finite differences.\n\nThis function computes the directional derivative of f at x in direction v without explicitly forming the Jacobian matrix. This is more efficient than computing the full Jacobian when only J*v is needed.\n\nArguments\n\nf: Function to differentiate (vector→vector map)\nx::AbstractArray{<:Number}: Point at which to evaluate the Jacobian\nv::AbstractArray{<:Number}: Direction vector for the product\nfdtype::Type{T1}=Val{:forward}: Finite difference method (:forward, :central)\nf_in=nothing: Pre-computed f(x) value (if available)\n\nKeyword Arguments\n\nrelstep: Relative step size (default: method-dependent optimal value)\nabsstep=relstep: Absolute step size fallback\n\nReturns\n\nVector J(x) * v representing the Jacobian-vector product\n\nExamples\n\nf(x) = [x[1]^2 + x[2], x[1] * x[2], x[2]^3]\nx = [1.0, 2.0]\nv = [1.0, 0.0]  # Direction vector\njvp = finite_difference_jvp(f, x, v)  # Directional derivative\n\nMathematical Background\n\nThe JVP is computed using the finite difference approximation:\n\nForward: J(x) * v ≈ (f(x + h*v) - f(x)) / h\nCentral: J(x) * v ≈ (f(x + h*v) - f(x - h*v)) / (2h)\n\nwhere h is the step size and v is the direction vector.\n\nNotes\n\nRequires only 2 function evaluations (vs O(n) for full Jacobian)\nForward differences: 2 function evaluations, O(h) accuracy\nCentral differences: 2 function evaluations, O(h²) accuracy\nParticularly efficient when v is sparse or when only one directional derivative is needed\n\n\n\n\n\nFiniteDiff.finite_difference_jvp(\n    f,\n    x,\n    v,\n    cache::JVPCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"jvp/#FiniteDiff.finite_difference_jvp!","page":"Jacobian-Vector Products","title":"FiniteDiff.finite_difference_jvp!","text":"finite_difference_jvp!(\n    jvp::AbstractArray{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    v::AbstractArray{<:Number},\n    fdtype     :: Type{T1}=Val{:forward},\n    returntype :: Type{T2}=eltype(x),\n    f_in       :: Union{T2,Nothing}=nothing;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCache-less.\n\n\n\n\n\nFiniteDiff.finite_difference_jvp!(\n    jvp::AbstractArray{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    v::AbstractArray{<:Number},\n    cache::JVPCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    dir=true)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"jvp/#Cache","page":"Jacobian-Vector Products","title":"Cache","text":"","category":"section"},{"location":"jvp/","page":"Jacobian-Vector Products","title":"Jacobian-Vector Products","text":"FiniteDiff.JVPCache","category":"page"},{"location":"jvp/#FiniteDiff.JVPCache","page":"Jacobian-Vector Products","title":"FiniteDiff.JVPCache","text":"JVPCache{X1, FX1, FDType}\n\nCache structure for Jacobian-vector product (JVP) computations.\n\nStores temporary arrays needed for efficient JVP computation without repeated allocations. The JVP computes J(x) * v where J(x) is the Jacobian of function f at point x  and v is a vector.\n\nFields\n\nx1::X1: Temporary array for perturbed input values\nfx1::FX1: Temporary array for function evaluations\n\n\n\n\n\n","category":"type"},{"location":"reproducibility/#Reproducibility","page":"-","title":"Reproducibility","text":"","category":"section"},{"location":"reproducibility/","page":"-","title":"-","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"</details>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"</details>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"</details>","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"You can also download the \n<a href=\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"reproducibility/","page":"-","title":"-","text":"\">project</a> file.","category":"page"},{"location":"utilities/#Internal-Utilities","page":"Internal Utilities","title":"Internal Utilities","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"Internal utility functions and algorithms used throughout FiniteDiff.jl. These functions are primarily for advanced users who need to understand or extend the library's functionality.","category":"page"},{"location":"utilities/#Array-Utilities","page":"Internal Utilities","title":"Array Utilities","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"Internal utilities for handling different array types and ensuring compatibility across the Julia ecosystem:","category":"page"},{"location":"utilities/#Type-Conversion-Functions","page":"Internal Utilities","title":"Type Conversion Functions","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"These functions help FiniteDiff.jl work with various array types including StaticArrays, structured matrices, and GPU arrays:","category":"page"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"_vec(x): Vectorizes arrays while preserving scalars unchanged\n_mat(x): Ensures matrix format, converting vectors to column matrices  \nsetindex(x, v, i...): Non-mutating setindex operations for immutable arrays","category":"page"},{"location":"utilities/#Hessian-Utilities","page":"Internal Utilities","title":"Hessian Utilities","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"Helper functions specifically for Hessian computation:","category":"page"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"_hessian_inplace(x): Determines whether to use in-place operations based on array mutability\n__Symmetric(x): Wraps matrices in Symmetric views for mathematical correctness\nmutable_zeromatrix(x): Creates mutable zero matrices compatible with input arrays","category":"page"},{"location":"utilities/#Sparse-Jacobian-Internals","page":"Internal Utilities","title":"Sparse Jacobian Internals","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"Graph coloring and sparse matrix algorithms for efficient Jacobian computation:","category":"page"},{"location":"utilities/#Matrix-Construction","page":"Internal Utilities","title":"Matrix Construction","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"_make_Ji(...): Constructs Jacobian contribution matrices for both sparse and dense cases\n_colorediteration!(...): Core loop for sparse Jacobian assembly using graph coloring\n_findstructralnz(A): Finds structural non-zero patterns in dense matrices","category":"page"},{"location":"utilities/#Sparsity-Detection","page":"Internal Utilities","title":"Sparsity Detection","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"_use_findstructralnz(sparsity): Determines when to use structural sparsity information\n_use_sparseCSC_common_sparsity(J, sparsity): Tests for common sparsity patterns between matrices","category":"page"},{"location":"utilities/#Performance-Optimizations","page":"Internal Utilities","title":"Performance Optimizations","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"fast_jacobian_setindex!(...): Optimized index operations for sparse Jacobian assembly\nvoid_setindex!(...): Wrapper for setindex operations that discards return values","category":"page"},{"location":"utilities/#JVP-Utilities","page":"Internal Utilities","title":"JVP Utilities","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"resize!(cache::JVPCache, i): Resizes JVP cache arrays for dynamic problems\nresize!(cache::JacobianCache, i): Resizes Jacobian cache arrays for dynamic problems","category":"page"},{"location":"utilities/#Error-Handling","page":"Internal Utilities","title":"Error Handling","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"fdtype_error(::Type{T}): Provides informative error messages for unsupported finite difference type combinations","category":"page"},{"location":"utilities/#Design-Philosophy","page":"Internal Utilities","title":"Design Philosophy","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"These internal utilities follow several design principles:","category":"page"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"Type Stability: All functions are designed for type-stable operations\nZero Allocation: Internal utilities avoid allocations when possible\nGenericity: Support for multiple array types (dense, sparse, static, GPU)\nPerformance: Optimized for the specific needs of finite difference computation\nSafety: Proper error handling and bounds checking where needed","category":"page"},{"location":"utilities/#Advanced-Usage","page":"Internal Utilities","title":"Advanced Usage","text":"","category":"section"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"These functions are primarily internal, but advanced users may find them useful for:","category":"page"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"Custom finite difference implementations\nIntegration with other differentiation libraries  \nPerformance optimization in specialized applications\nUnderstanding the implementation details of FiniteDiff.jl","category":"page"},{"location":"utilities/","page":"Internal Utilities","title":"Internal Utilities","text":"Most users should rely on the main API functions rather than calling these utilities directly.","category":"page"},{"location":"hessians/#Hessians","page":"Hessians","title":"Hessians","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Functions for computing Hessian matrices of scalar-valued functions.","category":"page"},{"location":"hessians/#Function-Requirements","page":"Hessians","title":"Function Requirements","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Hessian functions are designed for scalar-valued functions f(x) where:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"x is a vector of parameters\nf(x) returns a scalar value\nThe Hessian H[i,j] = ∂²f/(∂x[i]∂x[j]) is automatically symmetrized","category":"page"},{"location":"hessians/#Mathematical-Background","page":"Hessians","title":"Mathematical Background","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"For a scalar function f: ℝⁿ → ℝ, the Hessian central difference approximation is:","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"H[i,j] ≈ (f(x + eᵢhᵢ + eⱼhⱼ) - f(x + eᵢhᵢ - eⱼhⱼ) - f(x - eᵢhᵢ + eⱼhⱼ) + f(x - eᵢhᵢ - eⱼhⱼ)) / (4hᵢhⱼ)","category":"page"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"where eᵢ is the i-th unit vector and hᵢ is the step size in dimension i.","category":"page"},{"location":"hessians/#Performance-Considerations","page":"Hessians","title":"Performance Considerations","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"Complexity: Requires O(n²) function evaluations for an n-dimensional input\nAccuracy: Central differences provide O(h²) accuracy for second derivatives  \nMemory: The result is returned as a Symmetric matrix view\nAlternative: For large problems, consider computing the gradient twice instead","category":"page"},{"location":"hessians/#StaticArrays-Support","page":"Hessians","title":"StaticArrays Support","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"The cache constructor automatically detects StaticArray types and adjusts the inplace parameter accordingly for optimal performance.","category":"page"},{"location":"hessians/#Functions","page":"Hessians","title":"Functions","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"FiniteDiff.finite_difference_hessian\nFiniteDiff.finite_difference_hessian!","category":"page"},{"location":"hessians/#FiniteDiff.finite_difference_hessian","page":"Hessians","title":"FiniteDiff.finite_difference_hessian","text":"finite_difference_hessian(\n    f,\n    x::AbstractArray{<:Number},\n    fdtype::Type{T1}=Val{:hcentral},\n    inplace::Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCompute the Hessian matrix of scalar function f at point x using finite differences.\n\nThis is the cache-less version that allocates temporary arrays internally. The Hessian H[i,j] = ∂²f/(∂x[i]∂x[j]) is computed using the specified finite difference method, typically central differences for optimal accuracy.\n\nArguments\n\nf: Scalar-valued function to differentiate (vector→scalar map)\nx::AbstractArray{<:Number}: Point at which to evaluate the Hessian\nfdtype::Type{T1}=Val{:hcentral}: Finite difference method (:hcentral for Hessian central differences)\ninplace::Type{Val{T2}}: Whether to use in-place operations (auto-detected for StaticArrays)\n\nKeyword Arguments\n\nrelstep: Relative step size (default: method-dependent optimal value)\nabsstep=relstep: Absolute step size fallback\n\nReturns\n\nHessian matrix H where H[i,j] = ∂²f/(∂x[i]∂x[j])\nReturns Symmetric(H) view to enforce mathematical symmetry\n\nExamples\n\nf(x) = x[1]^2 + x[1]*x[2] + x[2]^2  # Quadratic function\nx = [1.0, 2.0]\nH = finite_difference_hessian(f, x)  # 2×2 Symmetric matrix\n\nMathematical Background\n\nFor a scalar function f: ℝⁿ → ℝ, the Hessian central difference approximation is:\n\nH[i,j] ≈ (f(x + eᵢhᵢ + eⱼhⱼ) - f(x + eᵢhᵢ - eⱼhⱼ) - f(x - eᵢhᵢ + eⱼhⱼ) + f(x - eᵢhᵢ - eⱼhⱼ)) / (4hᵢhⱼ)\n\nwhere eᵢ is the i-th unit vector and hᵢ is the step size in dimension i.\n\nNotes\n\nRequires O(n²) function evaluations for an n-dimensional input\nCentral differences provide O(h²) accuracy for second derivatives\nThe result is automatically symmetrized using Symmetric() wrapper\nFor large problems, consider using finite_difference_gradient twice instead\n\n\n\n\n\nfinite_difference_hessian(\n    f,\n    x,\n    cache::HessianCache{T,fdtype,inplace};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"hessians/#FiniteDiff.finite_difference_hessian!","page":"Hessians","title":"FiniteDiff.finite_difference_hessian!","text":"finite_difference_hessian!(\n    H::AbstractMatrix,\n    f,\n    x::AbstractArray{<:Number},\n    fdtype     :: Type{T1}=Val{:hcentral},\n    inplace    :: Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nCache-less.\n\n\n\n\n\nfinite_difference_hessian!(\n    H,\n    f,\n    x,\n    cache::HessianCache{T,fdtype,inplace};\n    relstep = default_relstep(fdtype, eltype(x)),\n    absstep = relstep)\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"hessians/#Cache","page":"Hessians","title":"Cache","text":"","category":"section"},{"location":"hessians/","page":"Hessians","title":"Hessians","text":"FiniteDiff.HessianCache","category":"page"},{"location":"hessians/#FiniteDiff.HessianCache","page":"Hessians","title":"FiniteDiff.HessianCache","text":"HessianCache(\n    xpp,\n    xpm,\n    xmp,\n    xmm,\n    fdtype::Type{T1}=Val{:hcentral},\n    inplace::Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false})\n\nNon-allocating cache constructor.\n\n\n\n\n\nHessianCache(\n    x,\n    fdtype::Type{T1}=Val{:hcentral},\n    inplace::Type{Val{T2}} = x isa StaticArray ? Val{true} : Val{false})\n\nAllocating cache constructor.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"EditURL = \"https://github.com/JuliaDiff/FiniteDiff.jl/blob/master/README.md\"","category":"page"},{"location":"#FiniteDiff","page":"Home","title":"FiniteDiff","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Join the chat at https://julialang.zulipchat.com #sciml-bridged) (Image: Global Docs)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: codecov) (Image: Build Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages) (Image: SciML Code Style)","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is for calculating derivatives, gradients, Jacobians, Hessians, etc. numerically. This library is for maximizing speed while giving a usable interface to end users in a way that specializes on array types and sparsity. Included is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fully non-allocating mutable forms for fast array support\nFully non-mutating forms for static array support\nColoring vectors for efficient calculation of sparse Jacobians\nGPU-compatible, to the extent that you can be with finite differencing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want the fastest versions, create a cache and repeatedly call the differencing functions at different x values (or with different f functions), while if you want a quick and dirty numerical answer, directly call a differencing function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For analogous sparse differentiation with automatic differentiation, see SparseDiffTools.jl.","category":"page"},{"location":"#FiniteDiff.jl-vs-FiniteDifferences.jl","page":"Home","title":"FiniteDiff.jl vs FiniteDifferences.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FiniteDiff.jl and FiniteDifferences.jl are similar libraries: both calculate approximate derivatives numerically. You should definitely use one or the other, rather than the legacy Calculus.jl finite differencing, or reimplementing it yourself. At some point in the future they might merge, or one might depend on the other. Right now here are the differences:","category":"page"},{"location":"","page":"Home","title":"Home","text":"FiniteDifferences.jl supports basically any type, where as FiniteDiff.jl supports only array-ish types\nFiniteDifferences.jl supports higher order approximation\nFiniteDiff.jl is carefully optimized to minimize allocations\nFiniteDiff.jl supports coloring vectors for efficient calculation of sparse Jacobians","category":"page"},{"location":"#General-structure","page":"Home","title":"General structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The general structure of the library is as follows. You can call the differencing functions directly and this will allocate a temporary cache to solve the problem with. To make this non-allocating for repeat calls, you can call the cache construction functions. Each cache construction function has two possibilities: one version where you give it prototype arrays and it generates the cache variables, and one fully non-allocating version where you give it the cache variables. This is summarized as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Just want a quick derivative? Calculating once? Call the differencing function.\nGoing to calculate the derivative multiple times but don't have cache arrays around? Use the allocating cache and then pass this into the differencing function (this will allocate only in the one cache construction).\nHave cache variables around from your own algorithm and want to re-use them in the differencing functions? Use the non-allocating cache construction and pass the cache to the differencing function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the documentation for details on the API.","category":"page"},{"location":"#Function-definitions","page":"Home","title":"Function definitions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In all functions, the inplace form is f!(dx,x) while the out of place form is dx = f(x).","category":"page"},{"location":"#Coloring-vectors","page":"Home","title":"Coloring vectors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Coloring vectors are allowed to be supplied to the Jacobian routines, and these are the directional derivatives for constructing the Jacobian. For example, an accurate NxN tridiagonal Jacobian can be computed in just 4 f calls by using colorvec=repeat(1:3,N÷3). For information on automatically generating coloring vectors of sparse matrices, see SparseDiffTools.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Hessian coloring support is coming soon!","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"Home","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can also download the \n<a href=\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"\">project</a> file.","category":"page"},{"location":"epsilons/#Step-Size-Selection-(Epsilons)","page":"Step Size Selection","title":"Step Size Selection (Epsilons)","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"Functions and theory for computing optimal step sizes in finite difference approximations.","category":"page"},{"location":"epsilons/#Theory","page":"Step Size Selection","title":"Theory","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"The choice of step size (epsilon) in finite difference methods is critical for accuracy. Too large a step leads to truncation error, while too small a step leads to round-off error. The optimal step size balances these two sources of error.","category":"page"},{"location":"epsilons/#Error-Analysis","page":"Step Size Selection","title":"Error Analysis","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"For a function f with bounded derivatives, the total error in finite difference approximations consists of:","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"Truncation Error: Comes from the finite difference approximation itself\nForward differences: O(h) where h is the step size\nCentral differences: O(h²)\nHessian central differences: O(h²) for second derivatives\nRound-off Error: Comes from floating-point arithmetic\nForward differences: O(eps/h) where eps is machine epsilon\nCentral differences: O(eps/h)","category":"page"},{"location":"epsilons/#Optimal-Step-Sizes","page":"Step Size Selection","title":"Optimal Step Sizes","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"Minimizing the total error truncation + round-off gives optimal step sizes:","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"Forward differences: h* = sqrt(eps) - balances O(h) truncation with O(eps/h) round-off\nCentral differences: h* = eps^(1/3) - balances O(h²) truncation with O(eps/h) round-off\nHessian central: h* = eps^(1/4) - balances O(h²) truncation for mixed derivatives\nComplex step: h* = eps - no subtractive cancellation, only limited by machine precision","category":"page"},{"location":"epsilons/#Adaptive-Step-Sizing","page":"Step Size Selection","title":"Adaptive Step Sizing","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"The step size computation uses both relative and absolute components:","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"epsilon = max(relstep * abs(x), absstep) * dir","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"This ensures:","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"Large values: Use relative step relstep * |x| for scale-invariant accuracy\nSmall values: Use absolute step absstep to avoid underflow\nDirection: Multiply by dir (±1) for forward differences","category":"page"},{"location":"epsilons/#Implementation","page":"Step Size Selection","title":"Implementation","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"The step size computation is handled by internal functions:","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"compute_epsilon(fdtype, x, relstep, absstep, dir): Computes the actual step size for a given finite difference method and input value\ndefault_relstep(fdtype, T): Returns the optimal relative step size for a given method and numeric type","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"These functions are called automatically by all finite difference routines, but understanding their behavior can help with custom implementations or debugging numerical issues.","category":"page"},{"location":"epsilons/#Special-Cases","page":"Step Size Selection","title":"Special Cases","text":"","category":"section"},{"location":"epsilons/#Complex-Step-Differentiation","page":"Step Size Selection","title":"Complex Step Differentiation","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"For complex step differentiation, the step size is simply machine epsilon since this method avoids subtractive cancellation entirely:","category":"page"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"⚠️ Important: The function f must be complex analytic when the input is complex!","category":"page"},{"location":"epsilons/#Sparse-Jacobians","page":"Step Size Selection","title":"Sparse Jacobians","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"When computing sparse Jacobians with graph coloring, the step size is computed based on the norm of the perturbation vector to ensure balanced accuracy across all columns in the same color group.","category":"page"},{"location":"epsilons/#Practical-Considerations","page":"Step Size Selection","title":"Practical Considerations","text":"","category":"section"},{"location":"epsilons/","page":"Step Size Selection","title":"Step Size Selection","text":"Default step sizes are optimal for most smooth functions\nCustom step sizes may be needed for functions with unusual scaling or near-discontinuities\nRelative steps should scale with the magnitude of the input\nAbsolute steps provide a fallback for inputs near zero\nDirection parameter allows for one-sided differences when needed (e.g., at domain boundaries)","category":"page"},{"location":"gradients/#Gradients","page":"Gradients","title":"Gradients","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"Functions for computing gradients of scalar-valued functions with respect to vector inputs.","category":"page"},{"location":"gradients/#Function-Types","page":"Gradients","title":"Function Types","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"Gradients support two types of function mappings:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"Vector→scalar: f(x) where x is a vector and f returns a scalar\nScalar→vector: f(fx, x) for in-place evaluation or fx = f(x) for out-of-place","category":"page"},{"location":"gradients/#Performance-Notes","page":"Gradients","title":"Performance Notes","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"Forward differences: O(n) function evaluations, O(h) accuracy\nCentral differences: O(2n) function evaluations, O(h²) accuracy\nComplex step: O(n) function evaluations, machine precision accuracy","category":"page"},{"location":"gradients/#Cache-Management","page":"Gradients","title":"Cache Management","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"When using GradientCache with pre-computed function values:","category":"page"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"If you provide fx, then fx will be used in forward differencing to skip a function call\nYou must update cache.fx before each call to finite_difference_gradient!\nFor immutable types (scalars, StaticArray), use @set from Setfield.jl\nConsider aliasing existing arrays into the cache for memory efficiency","category":"page"},{"location":"gradients/#Functions","page":"Gradients","title":"Functions","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"FiniteDiff.finite_difference_gradient\nFiniteDiff.finite_difference_gradient!","category":"page"},{"location":"gradients/#FiniteDiff.finite_difference_gradient","page":"Gradients","title":"FiniteDiff.finite_difference_gradient","text":"FiniteDiff.finite_difference_gradient(\n    f,\n    x,\n    fdtype::Type{T1}=Val{:central},\n    returntype::Type{T2}=eltype(x),\n    inplace::Type{Val{T3}}=Val{true};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep,\n    dir=true)\n\nCompute the gradient of function f at point x using finite differences.\n\nThis is the cache-less version that allocates temporary arrays internally. Supports both vector→scalar maps f(x) → scalar and scalar→vector maps depending on the inplace parameter and function signature.\n\nArguments\n\nf: Function to differentiate\nIf typeof(x) <: AbstractArray: f(x) should return a scalar (vector→scalar gradient)\nIf typeof(x) <: Number and inplace=Val(true): f(fx, x) modifies fx in-place (scalar→vector gradient)\nIf typeof(x) <: Number and inplace=Val(false): f(x) returns a vector (scalar→vector gradient)\nx: Point at which to evaluate the gradient (vector or scalar)\nfdtype::Type{T1}=Val{:central}: Finite difference method (:forward, :central, :complex)\nreturntype::Type{T2}=eltype(x): Element type of gradient components\ninplace::Type{Val{T3}}=Val{true}: Whether to use in-place function evaluation\n\nKeyword Arguments\n\nrelstep: Relative step size (default: method-dependent optimal value)\nabsstep=relstep: Absolute step size fallback\ndir=true: Direction for step size (typically ±1)\n\nReturns\n\nGradient vector ∇f where ∇f[i] = ∂f/∂x[i]\n\nExamples\n\n# Vector→scalar gradient\nf(x) = x[1]^2 + x[2]^2\nx = [1.0, 2.0]\ngrad = finite_difference_gradient(f, x)  # [2.0, 4.0]\n\n# Scalar→vector gradient (out-of-place)\ng(t) = [t^2, t^3]\nt = 2.0\ngrad = finite_difference_gradient(g, t, Val(:central), eltype(t), Val(false))\n\nNotes\n\nForward differences: O(n) function evaluations, O(h) accuracy\nCentral differences: O(2n) function evaluations, O(h²) accuracy\nComplex step: O(n) function evaluations, machine precision accuracy\n\n\n\n\n\nFiniteDiff.finite_difference_gradient!(\n    df::AbstractArray{<:Number},\n    f,\n    x::AbstractArray{<:Number},\n    cache::GradientCache;\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep\n    dir=true)\n\nGradients are either a vector->scalar map f(x), or a scalar->vector map f(fx,x) if inplace=Val{true} and fx=f(x) if inplace=Val{false}.\n\nCached.\n\n\n\n\n\n","category":"function"},{"location":"gradients/#FiniteDiff.finite_difference_gradient!","page":"Gradients","title":"FiniteDiff.finite_difference_gradient!","text":"FiniteDiff.finite_difference_gradient!(\n    df,\n    f,\n    x,\n    fdtype::Type{T1}=Val{:central},\n    returntype::Type{T2}=eltype(df),\n    inplace::Type{Val{T3}}=Val{true};\n    relstep=default_relstep(fdtype, eltype(x)),\n    absstep=relstep)\n\nGradients are either a vector->scalar map f(x), or a scalar->vector map f(fx,x) if inplace=Val{true} and fx=f(x) if inplace=Val{false}.\n\nCache-less.\n\n\n\n\n\n","category":"function"},{"location":"gradients/#Cache","page":"Gradients","title":"Cache","text":"","category":"section"},{"location":"gradients/","page":"Gradients","title":"Gradients","text":"FiniteDiff.GradientCache","category":"page"},{"location":"gradients/#FiniteDiff.GradientCache","page":"Gradients","title":"FiniteDiff.GradientCache","text":"FiniteDiff.GradientCache(\n    df         :: Union{<:Number,AbstractArray{<:Number}},\n    x          :: Union{<:Number, AbstractArray{<:Number}},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(df),\n    inplace    :: Type{Val{T3}} = Val{true})\n\nAllocating Cache Constructor\n\n\n\n\n\nFiniteDiff.GradientCache(\n    fx         :: Union{Nothing,<:Number,AbstractArray{<:Number}},\n    c1         :: Union{Nothing,AbstractArray{<:Number}},\n    c2         :: Union{Nothing,AbstractArray{<:Number}},\n    c3         :: Union{Nothing,AbstractArray{<:Number}},\n    fdtype     :: Type{T1} = Val{:central},\n    returntype :: Type{T2} = eltype(fx),\n    inplace    :: Type{Val{T3}} = Val{true})\n\nNon-Allocating Cache Constructor\n\nArguments\n\nfx: Cached function call.\nc1, c2, c3: (Non-aliased) caches for the input vector.\nfdtype = Val(:central): Method for cmoputing the finite difference.\nreturntype = eltype(fx): Element type for the returned function value.\ninplace = Val(false): Whether the function is computed in-place or not.\n\nOutput\n\nThe output is a GradientCache struct.\n\njulia> x = [1.0, 3.0]\n2-element Vector{Float64}:\n 1.0\n 3.0\n\njulia> _f = x -> x[1] + x[2]\n#13 (generic function with 1 method)\n\njulia> fx = _f(x)\n4.0\n\njulia> gradcache = GradientCache(copy(x), copy(x), copy(x), fx)\nGradientCache{Float64, Vector{Float64}, Vector{Float64}, Vector{Float64}, Val{:central}(), Float64, Val{false}()}(4.0, [1.0, 3.0], [1.0, 3.0], [1.0, 3.0])\n\n\n\n\n\n","category":"type"}]
}
